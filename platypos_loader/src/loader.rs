use alloc::vec;
use core::hint::unreachable_unchecked;
use core::slice;

use goblin::elf64::header::{self, Header};
use goblin::elf64::program_header::{self, ProgramHeader};
use plain::Plain;

use log::{debug, warn};

use uefi::prelude::*;
use uefi::proto::media::file::{FileType, RegularFile};
use uefi::table::boot::MemoryType;

use x86_64::registers::control::{Cr3, Cr3Flags, Efer, EferFlags};
use x86_64::structures::paging::{
    MappedPageTable, Mapper, Page, PageTable, PageTableFlags, PhysFrame,
    Size4KiB,
};
use x86_64::{PhysAddr, VirtAddr};

use crate::filesystem::locate_file;
use crate::util::allocator::UefiPageAllocator;
use crate::util::page_table::clone_pml4;

// Use distinct memory types for the kernel code, page tables, etc. instead of LOADER_DATA
// so that the kernel can reclaim the rest of the memory used for the loader

/// Memory containing the kernel binary, which cannot be reclaimed
const KERNEL_MEMORY_TYPE: MemoryType = MemoryType(0x70000042);

/// Memory containing kernel data, such as its initial stack, which the loader allocated.
const KERNEL_DATA_MEMORY_TYPE: MemoryType = MemoryType(0x70000043);

/// Memory containing the kernel page tables generated by the loader. These can be reclaimed once
/// the kernel has created its own page tables.
///
/// Note that these page tables include mappings for the loader, UEFI boot services, and UEFI
/// runtime services
const KERNEL_PAGE_TABLE_MEMORY_TYPE: MemoryType = MemoryType(0x70000044);

// Virtual address range for the kernel stack
const KERNEL_STACK_LOW: u64 = 0xffffffff71000000;
const KERNEL_STACK_HIGH: u64 = 0xffffffff71001000;

/// Load and launch the kernel executable at the specified path.
///
/// # Panics
/// If unable to load or start the kernel
pub fn launch_kernel(handle: uefi::Handle, system_table: SystemTable<Boot>, kernel_path: &[&str]) {
    // First, find the ELF binary
    let file = locate_file(system_table.boot_services(), kernel_path)
        .expect_success("Could not locate kernel")
        .expect("Kernel not found");

    if let FileType::Regular(mut file) = file.into_type().unwrap_success() {
        let mut allocator = UefiPageAllocator::new(system_table.boot_services(), KERNEL_PAGE_TABLE_MEMORY_TYPE);

        let (pml4, pml4_addr) = copy_uefi_page_table(&mut allocator);

//        let pml4_addr = allocator
//            .allocate_pages(KERNEL_PAGE_TABLE_MEMORY_TYPE, 1)
//            .expect("Could not allocate kernel PML4");
//        let pml4: &mut PageTable = unsafe { &mut *VirtAddr::new(pml4_addr.as_u64()).as_mut_ptr() };
//        pml4.zero();

        let entry_addr = load_kernel(&mut file, &mut allocator, pml4);

        // Add the boot loader to the kernel page table, so we can switch to it without crashing
//        map_loader(system_table.boot_services(), &mut allocator, pml4);

        create_kernel_stack(&mut allocator, pml4);

        unsafe { activate_page_table(pml4_addr) };
//        exit_boot_services(handle, system_table);
        unsafe { switch_to_kernel(entry_addr) };

    } else {
        panic!("Found a directory at expected kernel location");
    }
}

fn copy_uefi_page_table(allocator: &mut UefiPageAllocator) -> (&'static mut PageTable, PhysAddr) {
    // Clone the UEFI page table directly, since the memory map doesn't necessarily capture everything
    let (current_pml4_addr, _) = Cr3::read();
    let current_pml4 = unsafe { &* (current_pml4_addr.start_address().as_u64() as usize as *const PageTable) };
    clone_pml4(allocator, current_pml4)
}

/// Read the kernel ELF binary into memory, adding its segments to the page table. Returns the
/// entry point address of the kernel
fn load_kernel(
    file: &mut RegularFile,
    allocator: &mut UefiPageAllocator,
    pml4: &mut PageTable,
) -> VirtAddr {
    // Use random access since we don't want to read the whole kernel in yet. Instead, we'll go
    // section-by-section.

    file.set_position(0).unwrap_success();
    assert_eq!(file.get_position().unwrap_success(), 0);

    let header = read_header(file);
    debug!("Kernel start address is {:#x}", header.e_entry);

    file.set_position(header.e_phoff).unwrap_success();
    let mut ph_buf = vec![0u8; header.e_phnum as usize * program_header::SIZEOF_PHDR];
    assert_eq!(file.read(&mut ph_buf).unwrap_success(), ph_buf.len());
    for i in 0..header.e_phnum as usize {
        let ph = ProgramHeader::from_bytes(
            &ph_buf[i * program_header::SIZEOF_PHDR..(i + 1) * program_header::SIZEOF_PHDR],
        )
        .expect("Could not parse program header");

        map_segment(file, &ph, allocator, pml4);
    }

    VirtAddr::new(header.e_entry)
}

fn read_header(file: &mut RegularFile) -> Header {
    let mut header_buf = [0u8; header::SIZEOF_EHDR];

    assert_eq!(
        file.read(&mut header_buf).unwrap_success(),
        header::SIZEOF_EHDR
    );
    let header = Header::from_bytes(&header_buf);

    // Validate the header
    assert_eq!(
        &header.e_ident[0..header::SELFMAG],
        header::ELFMAG,
        "Invalid ELF magic"
    );
    assert_eq!(
        header.e_ident[header::EI_CLASS],
        header::ELFCLASS64,
        "Not a 64-bit ELF file"
    );
    assert_eq!(
        header.e_machine,
        header::EM_X86_64,
        "Kernel is not an x86-64 binary"
    );
    assert_eq!(
        header.e_type,
        header::ET_EXEC,
        "Kernel is not an executable"
    );
    debug!("Kernel header is valid");

    *header
}

/// Load a segment of the kernel into memory and add it to the kernel's page table
fn map_segment(
    file: &mut RegularFile,
    header: &ProgramHeader,
    allocator: &mut UefiPageAllocator,
    pml4: &mut PageTable,
) {
    if header.p_type != program_header::PT_LOAD {
        warn!(
            "Skipping {} segment",
            program_header::pt_to_str(header.p_type)
        );
        return;
    }

    let pages = (header.p_memsz as usize + 4095) / 4096;
    let allocation = allocator
        .allocate_pages(KERNEL_MEMORY_TYPE, pages)
        .expect("Could not allocate memory for segment");
    let buffer: &mut [u8] = unsafe {
        slice::from_raw_parts_mut(
            VirtAddr::new(allocation.as_u64()).as_mut_ptr(),
            pages * 4096,
        )
    };

    let data_offset = header.p_vaddr as usize % 4096; // Segment start might not be page-aligned
    file.set_position(header.p_offset)
        .expect_success("Could not seek to segment");
    assert_eq!(
        file.read(&mut buffer[data_offset..data_offset + header.p_filesz as usize])
            .expect_success("Could not read segment"),
        header.p_filesz as usize
    );

    // Zero out anything before and after the on-disk parts of the segment
    for e in &mut buffer[..data_offset] {
        *e = 0;
    }

    for e in &mut buffer[data_offset + header.p_filesz as usize..] {
        *e = 0;
    }

    let virt_start: Page<Size4KiB> = Page::containing_address(VirtAddr::new(header.p_vaddr));
    let phys_start = PhysFrame::containing_address(allocation);

    let mut mapper = unsafe { MappedPageTable::new(pml4, identity_translator) };
    let mut flags = PageTableFlags::PRESENT;
    if header.p_flags & program_header::PF_W != 0 {
        flags |= PageTableFlags::WRITABLE;
    }
    if header.p_flags & program_header::PF_X == 0 {
        flags |= PageTableFlags::NO_EXECUTE;
    }

    for i in 0..pages as u64 {
        unsafe {
            mapper
                .map_to(virt_start + i, phys_start + i, flags, allocator)
                .expect("Could not map segment")
                .ignore()
        };
    }

    debug!(
        "Loaded segment starting at {:#x} at physical address {:#x}",
        header.p_vaddr, allocation
    );
}

fn map_loader(
    boot_services: &BootServices,
    allocator: &mut UefiPageAllocator,
    pml4: &mut PageTable,
) {
    let mut mapper = unsafe { MappedPageTable::new(pml4, identity_translator) };

    // Make the buffer a bit larger than specified, since allocating it may add to the memory map
    let mut buf = vec![0u8; boot_services.memory_map_size() + 256];
    let (_, memory_map) = boot_services
        .memory_map(&mut buf)
        .expect_success("Could not get memory map");

    for entry in memory_map {
        if entry.ty == MemoryType::LOADER_DATA
            || entry.ty == MemoryType::LOADER_CODE
            || entry.ty == MemoryType::BOOT_SERVICES_CODE
            || entry.ty == MemoryType::BOOT_SERVICES_DATA
            || entry.ty == MemoryType::RUNTIME_SERVICES_CODE
            || entry.ty == MemoryType::RUNTIME_SERVICES_DATA
        {
            debug!(
                "Adding {}-page {:?} mapping at {:#x}",
                entry.page_count, entry.ty, entry.phys_start
            );


            // Identity-map everything - the virt_start field seems to always be 0
            assert_eq!(entry.virt_start, 0);
            let phys_start: PhysFrame<Size4KiB> =
                PhysFrame::from_start_address(PhysAddr::new(entry.phys_start))
                    .expect("Memory region is not page-aligned");
            let virt_start = Page::from_start_address(VirtAddr::new(entry.phys_start)).unwrap();

            for i in 0..entry.page_count {
                unsafe {
                    mapper
                        .map_to(
                            virt_start + i,
                            phys_start + i,
                            PageTableFlags::PRESENT | PageTableFlags::WRITABLE,
                            allocator,
                        )
                        .unwrap()
                        .ignore()
                };
            }
        }
    }
}

fn create_kernel_stack(allocator: &mut UefiPageAllocator, pml4: &mut PageTable) {
    let pages = (KERNEL_STACK_HIGH - KERNEL_STACK_LOW) as usize / 4096;
    let phys_addr = allocator
        .allocate_pages(KERNEL_DATA_MEMORY_TYPE, pages)
        .expect("Could not allocate kernel stack");

    let mut mapper = unsafe { MappedPageTable::new(pml4, identity_translator) };
    let virt_start: Page<Size4KiB> = Page::containing_address(VirtAddr::new(KERNEL_STACK_LOW));
    let phys_start = PhysFrame::containing_address(phys_addr);

    for i in 0..pages as u64 {
        unsafe {
            mapper
                .map_to(
                    virt_start + i,
                    phys_start + i,
                    PageTableFlags::PRESENT | PageTableFlags::WRITABLE | PageTableFlags::NO_EXECUTE,
                    allocator,
                )
                .expect("Could not map kernel stack")
                .ignore()
        };
    }

    // Zero out the stack for safety
    let stack: *mut u8 = VirtAddr::new(phys_addr.as_u64()).as_mut_ptr();
    unsafe {
        stack.write_bytes(0, pages * 4096);
    }

    debug!("Allocated kernel stack at {:?} ({} pages)", phys_addr, pages);
}

/// Switch to the given page table. The table must have mappings for the loader to continue to
/// operate.
unsafe fn activate_page_table(pml4_addr: PhysAddr) {
    debug!("Switching to kernel page tables at {:#x}", pml4_addr);

    // Enable the no-execute flag used in the kernel page table
    Efer::update(|efer| *efer |= EferFlags::NO_EXECUTE_ENABLE);

    debug!("A!");
    debug!("B");
    let frame = PhysFrame::from_start_address(pml4_addr).expect("PML4 is not page-aligned");
    debug!("C: {:?}", frame);

    Cr3::write(frame,Cr3Flags::empty());

    debug!("HI!");
}

fn exit_boot_services(handle: uefi::Handle, system_table: SystemTable<Boot>) {
    debug!("Exiting UEFI boot services");

    // TODO: allocate and then return boot info structure
    let mut memory_map_buffer = vec![0u8; system_table.boot_services().memory_map_size() + 256];

    let (runtime_table, memory_map) = system_table.exit_boot_services(handle, &mut memory_map_buffer).expect_success("Could not exit boot services");
}

unsafe fn switch_to_kernel(entry_addr: VirtAddr) -> ! {
    asm!("pushq $0\n\t\
          retq\n\t\
          hlt" : : "r"(entry_addr), "{rsp}"(KERNEL_STACK_HIGH) : "memory" : "volatile");
    unreachable_unchecked();
}

/// Identity physical-to-virtual translator for MappedPageTable
fn identity_translator(frame: PhysFrame) -> *mut PageTable {
    VirtAddr::new(frame.start_address().as_u64()).as_mut_ptr()
}

